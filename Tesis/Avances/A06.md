---
Fecha: 2025-08-03
---
# Regresión Simbólica: Gas Ideal

Uno de los objetivos de este trabajo es encontrar la función de partición $Z$ a partir de expresiones dadas de algunas cantidades termodinámicas de un agujero negro, estas pueden ser $P$ o $S$ por ejemplo.
En los avances anteriores encontramos $Z$ "manualmente", ahora, podría resultar enriquecedor ver que pueden hacer los algoritmos de inteligencia artificial en este problema. Para ello simplificaremos el problema volviendo desde a los agujeros negros a los gases clásicos y empezaremos con el gas ideal, que usaremos como punto de partida para probar la idea principal. Nos olvidaremos que conocemos $Z$ y solo usaremos la información de $V$, $T$, $P$, etc, simulando la situación que tenemos en el caso del agujero negro donde solo conocemos las expresiones algunas cantidades termodinámicas.
## Idea General

La estrategia para encontrar $Z$ del gas ideal se basa en el uso del teorema de aproximación universal de las redes neuronales, que nos dice que ciertas arquitecturas de redes neuronales pueden aproximar cualquier función (con ciertas condiciones), para nuestro problema la función que queremos aproximar es $Z_{\text{Real}}$ mediante una red neuronal $\text{NN}$, que también es una una función con algunas bondades que hacen que sean relativamente sencillas de manipular con software y hardware moderno.
La siguiente pregunta que emerge naturalmente al entrenar este tipo de algoritmos es sobre el origen de los datos. Comunmente se tiene un poco de información de la función que se quiere aproximar, pero en este caso no conocemos nada de $Z$. No esta todo perdido ya que tenemos acceso a la presión $P$, la entropía $S$ y la energía $E$ del gas. Cantidades que se relacionan con $Z$ por medio de operadores como derivadas, logaritmos, entre otros. Podemos aprovechar este hecho en las función de perdida para poder entrenar nuestra red neuronal con las cantidades conocidas, es decir no entrenamos nuestra red neuronal directamente con los datos de la función que queremos aproximar si no con datos que están relacionados a ella mediante otras funciones, esto es conocido como Scientific Machine Learning (SciML). 
Una vez entrenada la red neuronal obtendríamos que $\text{NN} \approx Z$, si queremos interpretar los resultados debemos ser capaces de escribir $\text{NN}$ como una expresión algebraica relativamente sencilla para leer de ella las propiedades del sistema, como conmunmente se hace en Mecánica Estadística. Debido a las arquitecturas típicas de las redes neuronales las expresiones algebraicas que las representan son muy complejas y poco podemos obtener de ellas. Para solucionar esto usaremos algoritmos de regresión simbólica que buscan las expresiones mas simples que puedan explicar los datos de manera interpretable. Es decir buscamos una nueva función $\text{NN}_\text{reg}$ tal que  $\text{NN}_\text{reg} \approx \text{NN} \approx Z$ que explique de la forma mas simple pero no simplista los datos.

> [!Note] En resumen:
> Usaremos a una red neuronal como una aproximación de $Z$ y de esta extraeremos una expresión algebraica con regresión simbólica.
## Variables independientes

Siendo mas específicos queremos encontrar la función de partición $Z$ del ensamble canónico. Recordando que este ensamble describe un sistema en equilibrio térmico con un reservorio de temperatura constante $T$, pero con volumen $V$ y número de partículas $N$ fijos. 
Elegimos $T$, $V$ y $N$ como nuestras variables independientes, esto significa que $\text{NN}= \text{NN}(V,T,N)$. 

## Entrenamiento de la red neuronal

Como habíamos propuesto, para probar si la idea funciona de la búsqueda de $Z$ con inteligencia artificial, imaginaremos que únicamente conocemos las expresiones para $P$, $S$ y $E$ en términos de otras cantidades termodinámicas y no sabemos nada de $Z$.

> [!Warning] Importante:
> De manera simplificada el entrenamiento de una red neuronal $\text{NN}(x_1,\cdots;\omega_1,\cdots)$  se reduce a ajustar el conjunto de parámetros $\{\omega_1, \cdots \}$ tal que minimicen una función de pérdida $\text{Loss}$. Es decir entrenar una red neuronal es un problema de minimización. Una vez finalizado el entrenamiento obtendremos que $NN \approx Z$

Ya que la red neuronal $\text{NN}$ es tratada como una aproximación de la $Z$ podemos obtener de ella cantidades termodinámicas que denotamos como $P_\text{NN}$, $S_\text{NN}$, etc. La función de perdida de nuestra red neuronal que buscamos que sea minima es mas o menos $\text{Loss} = |P - P_\text{NN}| + |S - S_\text{NN}| + \cdots$. De manera intuitiva buscamos que las cantidades que predice nuestra red neuronal sean lo mas parecidas a las cantidades que si conocemos.

##  Problemas numéricos

Uno de los problemas mas comunes cuando queremos aproximar funciones del mundo de la física con redes neuronales son las constantes. 
Estas aveces tienen valores muy grandes o muy pequeños comparados con los que un computador puede manipular sin problemas de redondeo. Lo cual puede complicar el entrenamiento de una red neuronal o hacerlo inviable. Para solucionar esto es mejor deshacernos de las constantes haciendo un **re-escalado de las variables** de nuestro sistema de una manera conveniente para que las expresiones con las que trabajemos únicamente contengan las nuevas variables sin constantes.

La función de partición por lo general incluye términos exponenciales lo cual causaría inestabilidad numérica en los gradientes que utiliza el descenso del gradiente en el entrenamiento de la red neuronal. Para ello en lugar de aproximar $Z$ buscaremos aproximar una versión re-escalada de $\ln{Z}$. El logaritmo hará que las cantidades grandes sean mapeadas a una mas pequeñas.

El numero de partículas del sistema $N$ también puede ser visto como una constante problemática para cálculos numéricos por cual podemos refinar aun mas nuestra búsqueda y elegir convenientemente aproximar $\frac{\ln{Z}}{N}$ en lugar de $\ln{Z}$. Entonces realmente debemos decir que  $\text{NN}(V,T)$. 

> [!important] Importante:
 > Al finalizar el entrenamiento $\text{NN}(V,T) \approx \frac{\ln{Z}}{N}$

## Re-escalamiento de cantidades

Usando las constantes del sistema $h, m, K_B, N$ podemos definir constantes $T_0$, $P_0$ y $V_0$ con las que podemos definir las cantidades termodinámicas re-escaladas del sistemas:

$$
\hat{T} = \frac{T}{T_0} \quad \text{con} \quad T_0 = \frac{\mathcal{E}}{K_B} \quad \text{donde} \quad \mathcal{E} = \frac{h^2}{2 \pi m L^2}
$$
$$
\hat{P} = \frac{P}{P_0} \quad \text{con} \quad P_0 = \frac{\mathcal{E}}{L^3}
$$
$$
\hat{V} = \frac{V}{V_0} \quad \text{con} \quad V_0 = N L^3
$$
Donde $L^3$ es un volumen característico cualquiera del sistema.
También definimos:
$$
\hat{S} = \frac{S}{S_0} \quad \text{con} \quad S_0 = N K_B
$$
$$
\hat{E} = \frac{E}{E_0} \quad \text{con} \quad E_0 = N K_B T_0
$$
### $P$, $S$ y $E$ re-escaladas

Para probar la idea acordamos que de alguna manera únicamente conocemos $P$, $S$ y $E$, en función de otras cantidades termodinámicas, para el gas ideal estas son:
$$
P = \frac{N K_B T}{V}
$$
$$
S = N K_B \left( \ln{\left(\frac{V}{N}\right)} - \frac{3}{2}\ln{(\lambda^2)} + \frac{5}{2}\right)
$$
$$
E = \frac{3}{2} N K_B T
$$
Usando las variables definidas anteriormente podemos reescribir estas cantidades.
#### Presión

Partiendo de:
$$
\begin{align}
P &= \frac{N K_B T}{V} \\
&= \frac{N K_B \hat{T} T_0}{\hat{V} V_0} \\
&= \frac{N K_B \hat{T} \frac{\mathcal{E}}{K_B}}{\hat{V} N L^3} \\
& = \frac{\hat{T} \mathcal{E}}{\hat{V} L^3}
\end{align}
$$
Reordenando:

$$
P \frac{L^3}{\mathcal{E}} = \frac{\hat{T}}{\hat{V}}
$$
Entonces:
$$
\hat{P} = \frac{\hat{T}}{\hat{V}}
$$
#### Entropía

Partiendo de:
$$
S = N K_B \left( \ln{\left(\frac{V}{N}\right)} - \frac{3}{2}\ln{(\lambda^2)} + \frac{5}{2}\right)
$$
Usando que $\lambda = \frac{h}{\sqrt{2 \pi m K_B T}}$
$$
\begin{align}
S &= N K_B \left( \ln{\left(\frac{V}{N}\right)} - \frac{3}{2}\ln{\left(\frac{h^2}{2\pi m K_B T }\right)} + \frac{5}{2}\right) \\

&= N K_B \left( \ln{\left(\frac{\hat{V} V_0}{N}\right)} - \frac{3}{2}\ln{\left(\frac{h^2}{2\pi m K_B \hat{T} T_0}\right)} + \frac{5}{2}\right) \\

&= N K_B \left( \ln{\left(\frac{\hat{V} NL^3}{N}\right)} - \frac{3}{2}\ln{\left(\frac{h^2}{2\pi m K_B \hat{T} \frac{h^2}{2 \pi m L^2 K_B}}\right)} + \frac{5}{2}\right) \\

&= N K_B \left( \ln{\left(\hat{V}L^3\right)} - \frac{3}{2}\ln{\left(L^2\right)} + \frac{5}{2}\right) \\

&= N K_B \left(\ln{(\hat{V}L^3)} -3\ln{(L)} + \frac{5}{2}\right) \\

&= N K_B \left( \ln{(\hat{V})} + 3\ln{(L)} -3\ln{(L)} + \frac{5}{2}\right) \\

&= N K_B \left( \ln{(\hat{V})} + \frac{5}{2}\right)
\end{align}
$$
Reordenando:
$$
\frac{S}{N K_B} = \ln{\hat{V}}+ \frac{5}{2}
$$
Entonces:
$$
\hat{S} = \ln{\hat{V}} + \frac{5}{2}
$$
#### Energía

Partiendo de:
$$
\begin{align}
E &= \frac{3}{2} N K_B T \\
&= \frac{3}{2} N K_B \hat{T} T_0
\end{align}
$$
Reordenando:
$$
\frac{E}{N K_B T_0} = \frac{3}{2} \hat{T}
$$
Entonces:
$$
\hat{E} = \frac{3}{2} \hat{T}
$$
> [!note] En resumen:
> En las nuevas variables:
> $\hat{P} = \frac{\hat{T}}{\hat{V}}$
> $\hat{S} = \ln{\hat{V}} + \frac{5}{2}$
> $\hat{E} = \frac{3}{2} \hat{T}$
### $P$, $S$ y $E$ desde $Z$

En la sección [[A06#Entrenamiento]] mostramos que la función de perdida (la función que debemos minimizar) incluye términos con cantidades que conocemos como $P$ y también contiene $P_\text{NN}$. Estas ultimas cantidades deben ser calculadas a partir de la red neuronal $\text{NN}$, ya que es una aproximación de $\frac{\ln{Z}}{N}$ podemos calcularlas a partir de esta. Sin olvidar que para el entrenar nuestra red neuronal trabajamos con cantidades re-escaladas, entonces necesitamos $\hat{P}$, $\hat{S}$ y $\hat{E}$ en función de $\frac{\ln{Z}}{N}$. ​
#### Presión

Recordando que
$$
P = K_B T \frac{\partial}{\partial V} \left( \ln Z \right)
$$
Note que:
$$
\begin{align}
\frac{\partial}{\partial V} &= \frac{\partial \hat{V}}{\partial V} \frac{\partial}{\partial \hat{V}} \\

\frac{\partial}{\partial V} &= \frac{1}{V_0} \frac{\partial}{\partial \hat{V}} \\

\frac{\partial}{\partial V} &= \frac{1}{N L^3} \frac{\partial}{\partial \hat{V}}
\end{align}
$$
Entonces:

$$
P = K_B \hat{T} \frac{h^2}{2 \pi m L^2} \cdot \frac{1}{K_B} \cdot \frac{1}{N L^3} \frac{\partial}{\partial \hat{V}} \left( \ln Z \right)
$$
$$
P = \hat{T} \frac{h^2}{2 \pi m L^5} \frac{\partial}{\partial \hat{V}} \left( \frac{\ln Z}{N} \right)
$$
$$
\hat{P} \, P_0 = \hat{T} \, \frac{h^2}{2 \pi m L^5} \, \frac{\partial}{\partial \hat{V}} \left( \frac{\ln Z}{N} \right)
$$
$$
\hat{P} \left( \frac{h^2}{2 \pi m L^2} \right) \frac{1}{L^3} = \hat{T} \left( \frac{h^2}{2 \pi m L^5} \right) \frac{\partial}{\partial \hat{V}} \left( \frac{\ln Z}{N} \right)
$$
$$
\hat{P} = \hat{T} \, \frac{\partial}{\partial \hat{V}} \left( \frac{\ln Z}{N} \right)
$$
#### Entropía

Recordando que:
$$
S = -\frac{\partial F}{\partial T}
$$
$$
S = -\frac{\partial}{\partial T} \left( -K_B T \ln Z \right)
$$
$$
S = K_B \frac{\partial}{\partial T} \left( T \ln Z \right)
$$
$$
S = K_B \left( \ln Z + T \frac{\partial}{\partial T} \left( \ln Z \right) \right)
$$
$$
\frac{S}{K_B N} = \frac{\ln Z}{N} + T \frac{\partial}{\partial T} \left( \frac{\ln Z}{N} \right)
$$
Note que:
$$
\begin{align}
\frac{\partial}{\partial T} &= \frac{\partial \hat{T}}{\partial T} \frac{\partial}{\partial \hat{T}} \\

\frac{\partial}{\partial T} &= \frac{1}{T_0} \frac{\partial}{\partial \hat{T}} \\
\end{align}
$$
$$
\frac{S}{K_B N} = \frac{\ln Z}{N} + T \cdot \frac{1}{T_0} \frac{\partial}{\partial \hat{T}} \left( \frac{\ln Z}{N} \right)
$$
$$
\hat{S} = \frac{\ln Z}{N} + \hat{T} \frac{\partial}{\partial \hat{T}} \left( \frac{\ln Z}{N} \right)
$$
#### Energía

Recordando que:
$$
E = -\frac{\partial}{\partial \beta} \left( \ln Z \right)
$$
$$
E = -\frac{\partial \hat{T}}{\partial \beta} \frac{\partial}{\partial \hat{T}} \left( \ln Z \right)
$$
Note que:
$$
\hat{T} = \frac{T}{T_0} = \frac{K_B T}{K_B T_0} = \frac{1}{\beta K_B T_0}
$$
Entonces:
$$
\frac{\partial \hat{T}}{\partial \beta} = -\frac{1}{\beta^2 K_B T_0} = -\frac{K_B^2 T^2}{K_B T_0}
$$
$$
= -K_B \hat{T}^2 T = -K_B \hat{T}^2 T_0
$$

Por lo que:

$$
E = K_B \, \hat{T}^2 \, T_0 \, \frac{\partial}{\partial \hat{T}} \left( \ln Z \right)
$$
$$
\frac{E}{K_B T_0} = \hat{T}^2 \, \frac{\partial}{\partial \hat{T}} \left( \ln Z \right)
$$
$$
\frac{E}{N K_B T_0} = \hat{T}^2 \, \frac{\partial}{\partial \hat{T}} \left( \frac{\ln Z}{N} \right)
$$
Entonces:
$$
\hat{E} = \hat{T}^2 \, \frac{\partial}{\partial \hat{T}} \left( \frac{\ln Z}{N} \right)
$$
## Comprobación

Unicamente para evaluar que tan bien la red neuronal aprende la función de partición real $Z$ necesitamos reescribir esta en función de las variables re-escaladas.

Partiendo de:
$$
Z = \frac{1}{N!} \left( \frac{V}{\lambda^3} \right)^N
$$
$$
\ln{Z} = -\ln N! + N \ln \left( \frac{V}{\lambda^3} \right)
$$
$$
\ln{Z} = -N \ln N + N + N \ln \left( \frac{V}{\lambda^3} \right)
$$
$$
\frac{\ln{Z}}{N} = -\ln N + 1 + \ln \left( \frac{V}{\lambda^3} \right)
$$
Reemplazando las nuevas variables:
$$
\frac{\ln{Z}}{N} = -\ln N + 1 + \ln \left( \frac{\hat{V} V_0}{\lambda^3} \right)
$$
$$
\frac{\ln{Z}}{N} = -\ln N + 1 + \ln \left( \frac{\hat{V} N L^3}{\lambda^3} \right)
$$
$$
\frac{\ln{Z}}{N} = 1 + \ln \left( \hat{V} \frac{L^3}{\lambda^3} \right)
$$
Note que:
$$
\lambda = \frac{h}{\sqrt{2\pi m K_B T}}
$$
$$
\lambda = \frac{h}{\sqrt{2\pi m K_B \hat{T} T_0}}
$$
$$
\lambda = \frac{h}{\sqrt{2\pi m K_B \hat{T} \cdot \frac{h^2}{2\pi m L^2} \cdot \frac{1}{K_B}}}
$$
$$
\lambda = \frac{L}{\sqrt{\hat{T}}}
$$
Entonces:
$$
\frac{\ln{Z}}{N} = \ln \left( \frac{\hat{V} L^3}{\frac{L^3}{\hat{T}^{3/2}}} \right) + 1
$$
$$
\frac{\ln{Z}}{N} = \ln \left( \hat{V} \, \hat{T}^{3/2} \right) + 1
$$
Finalmente:
$$
\frac{\ln{Z}}{N} = \ln \hat{V} + \frac{3}{2} \ln \hat{T} + 1
$$
Esta expresión es la que esperamos que la red neuronal aprenda únicamente a partir de las expresiones de conocemos para $P$, $S$ y $E$. Esta expresión no puede ser "vista" por la red neuronal ya que en el caso del agujero negro no la conocemos.

Con esta adaptación del problema al mundo computacional podemos avanzar

## Resultados

